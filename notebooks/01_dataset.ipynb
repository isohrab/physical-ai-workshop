{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Guide to Load Dataset for Inference\n",
                "\n",
                "## Prerequisites and Key Concepts\n",
                "\n",
                "This notebook teaches you how to work with robot data. Robot data consists of recordings of what a robot sees (camera images) and does (arm movements, hand gestures) while completing tasks.\n",
                "\n",
                "**Key Terms:**\n",
                "- **Dataset**: A collection of robot demonstrations (like video recordings + robot movements)\n",
                "- **Modality**: Different types of data (video, robot joint positions, actions, language instructions)\n",
                "- **Embodiment**: The physical form/type of robot (humanoid, arm, mobile robot, etc.)\n",
                "- **Inference**: Using a trained AI model to predict what the robot should do next\n",
                "\n",
                "## Understanding Robot Data\n",
                "\n",
                "Robot datasets contain multiple types of information:\n",
                "1. **Video data**: What the robot \"sees\" through its cameras\n",
                "2. **State data**: Current positions of robot joints (arms, hands, etc.)\n",
                "3. **Action data**: What the robot should do next (move arm, close gripper, etc.)\n",
                "4. **Language data**: Human instructions (\"pick up the apple\")\n",
                "\n",
                "## LeRobot Format\n",
                "\n",
                "* This tutorial will show how to load data in LeRobot Format by using our dataloader. \n",
                "* We will use the `robot_sim.PickNPlace` dataset as an example which is already converted to LeRobot Format."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# any_describe: A utility function that prints detailed information about data structures\n",
                "# This helps you understand what's inside your data\n",
                "from gr00t.utils.misc import any_describe\n",
                "\n",
                "# LeRobotSingleDataset: The main class for loading robot datasets\n",
                "# Think of this as your \"data loader\" for robot demonstrations\n",
                "from gr00t.data.dataset import LeRobotSingleDataset\n",
                "\n",
                "# ModalityConfig: Tells the system which types of data to use (video, robot states, etc.)\n",
                "# This is like a \"menu\" where you select what data you want\n",
                "from gr00t.data.dataset import ModalityConfig\n",
                "\n",
                "# EmbodimentTag: Specifies what type of robot this data is from\n",
                "# Different robots need different handling (humanoid vs arm vs mobile robot)\n",
                "from gr00t.data.schema import EmbodimentTag"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading Your First Dataset\n",
                "\n",
                "Loading robot data is like opening a folder of robot demonstrations. We need to tell the system:\n",
                "1. **Where** to find the data (file path)\n",
                "2. **What** types of data to use (video, robot positions, etc.)\n",
                "3. **What** type of robot this data is from\n",
                "\n",
                "\n",
                "**Delta Indices Explained:**\n",
                "- `[0]` = Use only the current frame/timestep\n",
                "- `[-1, 0]` = Use the previous frame AND current frame\n",
                "- `[0, 1, 2]` = Use current frame and next 2 frames\n",
                "\n",
                "### Understanding Embodiment Tags\n",
                "\n",
                "**What is an Embodiment?** The physical form of the robot:\n",
                "- **Humanoid robot**: Has arms, legs, torso (like GR1)\n",
                "- **Robot arm**: Just an arm with a gripper\n",
                "- **Mobile robot**: Can move around\n",
                "\n",
                "**Why does this matter?** Different robots need different AI \"brains\". GR00T has specialized components for each robot type to get the best performance.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import gr00t\n",
                "\n",
                "\n",
                "REPO_PATH = os.path.dirname(os.path.dirname(gr00t.__file__))\n",
                "DATA_PATH = os.path.join(REPO_PATH, \"demo_data/robot_sim.PickNPlace\")\n",
                "\n",
                "# STEP 3: Show where we're loading data from\n",
                "print(\"Loading dataset... from\", DATA_PATH)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Configuring What Data to Use (Modality Configs)\n",
                "\n",
                "# WHAT: Tell the system exactly which types of robot data we want to use\n",
                "# WHY: Not all datasets have the same information, so we need to specify what's available\n",
                "# HOW: We create a \"configuration menu\" for each data type\n",
                "\n",
                "modality_configs = {\n",
                "    # === VIDEO CONFIGURATION ===\n",
                "    # WHAT: Configure camera/video data\n",
                "    # This tells the system to use the robot's main camera view\n",
                "    \"video\": ModalityConfig(\n",
                "        delta_indices=[0],  # [0] = only current frame (not previous/future frames)\n",
                "        modality_keys=[\"video.ego_view\"],  # \"ego_view\" = robot's main camera perspective\n",
                "    ),\n",
                "    \n",
                "    # === ROBOT STATE CONFIGURATION ===\n",
                "    # WHAT: Configure robot body position data (where all robot parts are positioned)\n",
                "    # This includes all the robot's body parts - arms, hands, legs, etc.\n",
                "    \"state\": ModalityConfig(\n",
                "        delta_indices=[0],  # [0] = current positions only\n",
                "        modality_keys=[\n",
                "            \"state.left_arm\",      # Left arm joint positions (7 joints: shoulder, elbow, wrist)\n",
                "            \"state.left_hand\",     # Left hand/gripper positions (6 finger joints)\n",
                "            \"state.left_leg\",      # Left leg joint positions\n",
                "            \"state.neck\",          # Neck/head joint positions\n",
                "            \"state.right_arm\",     # Right arm joint positions\n",
                "            \"state.right_hand\",    # Right hand/gripper positions\n",
                "            \"state.right_leg\",     # Right leg joint positions\n",
                "            \"state.waist\",         # Waist/torso joint positions\n",
                "        ],\n",
                "    ),\n",
                "    \n",
                "    # === ACTION CONFIGURATION ===\n",
                "    # WHAT: Configure what actions the robot should perform\n",
                "    # In this dataset, we only have hand actions (grasping, releasing)\n",
                "    \"action\": ModalityConfig(\n",
                "        delta_indices=[0],  # [0] = immediate next action\n",
                "        modality_keys=[\n",
                "            \"action.left_hand\",   # What the left hand should do (grip strength, finger positions)\n",
                "            \"action.right_hand\",  # What the right hand should do\n",
                "        ],\n",
                "    ),\n",
                "    \n",
                "    # === LANGUAGE CONFIGURATION ===\n",
                "    # WHAT: Configure human language instructions and validation\n",
                "    # This includes task descriptions (\"pick up the apple\") and quality labels\n",
                "    \"language\": ModalityConfig(\n",
                "        delta_indices=[0],  # [0] = current instruction\n",
                "        modality_keys=[\n",
                "            \"annotation.human.action.task_description\",  # Human task instruction\n",
                "            \"annotation.human.validity\"  # Whether this demonstration is good/bad\n",
                "        ],\n",
                "    ),\n",
                "}\n",
                "\n",
                "print(\"✅ Modality configuration created!\")\n",
                "print(f\"📊 We're using {len(modality_configs)} types of data: {list(modality_configs.keys())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Loading the Dataset - Putting It All Together\n",
                "\n",
                "# STEP 1: Choose the robot embodiment (what type of robot this data is from)\n",
                "# WHAT: EmbodimentTag.GR1 means this data is from a \"GR1\" humanoid robot\n",
                "# WHY: GR00T has different \"brains\" optimized for different robot types\n",
                "# Think of this as choosing the right \"driver\" for your robot hardware\n",
                "embodiment_tag = EmbodimentTag.GR1\n",
                "print(f\"🤖 Using embodiment: {embodiment_tag}\")\n",
                "print(\"   This tells GR00T to use its humanoid robot AI components\")\n",
                "\n",
                "# STEP 2: Load the dataset with all our configurations\n",
                "# WHAT: Create the dataset object that will handle loading robot data\n",
                "# HOW: We pass in the path, modality configs, and embodiment tag\n",
                "print(\"\\n🔄 Loading dataset...\")\n",
                "dataset = LeRobotSingleDataset(\n",
                "    DATA_PATH,           # Where to find the data\n",
                "    modality_configs,    # What types of data to use\n",
                "    embodiment_tag=embodiment_tag  # What type of robot this is\n",
                ")\n",
                "\n",
                "print('\\n'*2)\n",
                "print(\"=\"*100)\n",
                "print(f\"{' ✅ HUMANOID DATASET LOADED SUCCESSFULLY! ':=^100}\")\n",
                "print(\"=\"*100)\n",
                "print(f\"📊 Dataset contains {len(dataset)} data points (timesteps)\")\n",
                "print(f\"🎯 Each data point includes: video + robot states + actions + language\")\n",
                "\n",
                "# STEP 3: Examine a single data point to understand the structure\n",
                "# WHAT: Look at the 7th data point in the dataset\n",
                "# WHY: This helps us understand what information is available\n",
                "print(\"\\n🔍 Let's examine data point #7:\")\n",
                "resp = dataset[7]  # Get the 7th data point\n",
                "any_describe(resp)  # Print detailed information about this data point\n",
                "print(\"\\n📋 Available data keys:\", list(resp.keys()))\n",
                "print(\"\\n💡 Key explanations:\")\n",
                "print(\"   - Each 'key' represents a different type of information\")\n",
                "print(\"   - 'video.ego_view' = camera image from robot's perspective\")\n",
                "print(\"   - 'state.right_arm' = current positions of right arm joints\")\n",
                "print(\"   - 'action.right_hand' = what the right hand should do next\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualizing Robot Camera Data\n",
                "\n",
                "Let's look at what the robot actually \"sees\" through its camera during the demonstrations. This helps us understand the visual context of the robot's actions.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# WHAT: Extract and display robot camera images from different time points\n",
                "# WHY: Visual inspection helps us understand what the robot sees during tasks\n",
                "# HOW: We'll sample 10 images from the first 100 data points and display them in a grid\n",
                "\n",
                "import matplotlib.pyplot as plt  # For displaying images\n",
                "\n",
                "print(\"🎥 Extracting robot camera images...\")\n",
                "images_list = []  # Store the collected images\n",
                "\n",
                "# STEP 1: Collect images from every 10th data point (to see progression over time)\n",
                "# We're sampling images to see how the scene changes as the robot performs its task\n",
                "for i in range(100):  # Look at first 100 data points\n",
                "    if i % 10 == 0:  # Every 10th data point (0, 10, 20, 30, ...)\n",
                "        resp = dataset[i]  # Get data point i\n",
                "        img = resp[\"video.ego_view\"][0]  # Extract the camera image\n",
                "        # Note: [0] gets the current frame (remember delta_indices=[0])\n",
                "        images_list.append(img)\n",
                "        \n",
                "print(f\"✅ Collected {len(images_list)} images from the robot's camera\")\n",
                "\n",
                "# STEP 2: Display the images in a nice grid layout\n",
                "# Create a 2x5 grid to show all 10 images at once\n",
                "print(\"🖼️ Creating image grid...\")\n",
                "fig, axs = plt.subplots(2, 5, figsize=(20, 10))  # 2 rows, 5 columns\n",
                "\n",
                "# STEP 3: Place each image in the grid\n",
                "for i, ax in enumerate(axs.flat):  # axs.flat makes it easy to iterate through all subplots\n",
                "    ax.imshow(images_list[i])  # Display the image\n",
                "    ax.axis(\"off\")  # Hide axis numbers and ticks for cleaner look\n",
                "    ax.set_title(f\"Timestep {i*10}\", fontsize=12)  # Label with actual timestep\n",
                "    \n",
                "plt.tight_layout()  # Adjust spacing between images automatically\n",
                "plt.suptitle(\"🤖 Robot's Camera View During Pick & Place Task\", fontsize=16, y=1.02)\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n💡 What you're seeing:\")\n",
                "print(\"   - These are sequential frames from the robot's camera\")\n",
                "print(\"   - You can see how the scene changes as the robot performs its task\")\n",
                "print(\"   - Notice objects being picked up, moved, or manipulated\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Transformations - Preparing Data for AI Models\n",
                "\n",
                "Raw robot data needs to be \"processed\" before AI models can use it effectively. Think of this like preparing ingredients before cooking - we need to:\n",
                "- Resize images to standard sizes\n",
                "- Normalize numbers to consistent ranges  \n",
                "- Convert data to formats the AI model expects\n",
                "\n",
                "**Why do we need transformations?**\n",
                "- **AI models are picky**: They expect data in specific formats and ranges\n",
                "- **Consistency**: All images should be the same size, all numbers in similar ranges\n",
                "- **Performance**: Properly processed data helps models learn better and faster"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# WHAT: Import all the transformation functions we'll need\n",
                "# WHY: Each transform does a specific job to prepare the data for AI models\n",
                "from gr00t.data.transform.base import ComposedModalityTransform  # Combines multiple transforms\n",
                "from gr00t.data.transform import VideoToTensor, VideoCrop, VideoResize, VideoColorJitter, VideoToNumpy\n",
                "from gr00t.data.transform.state_action import StateActionToTensor, StateActionTransform\n",
                "from gr00t.data.transform.concat import ConcatTransform\n",
                "\n",
                "# STEP 1: Get our modality configurations (we defined these earlier)\n",
                "# We need to know which data streams to apply transforms to\n",
                "video_modality = modality_configs[\"video\"]    # Camera/video configuration\n",
                "state_modality = modality_configs[\"state\"]    # Robot body positions configuration  \n",
                "action_modality = modality_configs[\"action\"]  # Robot actions configuration\n",
                "\n",
                "print(\"🔧 Setting up data transformations...\")\n",
                "print(\"📝 Each transform will be applied in sequence:\")\n",
                "\n",
                "# STEP 2: Create a pipeline of transformations that will be applied to our data\n",
                "# Think of this as a factory assembly line - data goes through each step\n",
                "to_apply_transforms = ComposedModalityTransform(\n",
                "    transforms=[\n",
                "        # === VIDEO TRANSFORMATIONS ===\n",
                "        # Process camera images to make them suitable for AI models\n",
                "        \n",
                "        # 1. Convert images to tensors (AI-friendly format)\n",
                "        # Like converting a photo into a grid of numbers\n",
                "        VideoToTensor(apply_to=video_modality.modality_keys),\n",
                "        \n",
                "        # 2. Crop images slightly (remove 5% from edges)\n",
                "        # WHY: Focuses on center content, removes potential edge artifacts\n",
                "        VideoCrop(apply_to=video_modality.modality_keys, scale=0.95),\n",
                "        \n",
                "        # 3. Resize all images to 224x224 pixels (standard AI input size)\n",
                "        # WHY: AI models expect consistent input sizes\n",
                "        VideoResize(apply_to=video_modality.modality_keys, height=224, width=224, interpolation=\"linear\"),\n",
                "        \n",
                "        # 4. Add random color variations (data augmentation)\n",
                "        # WHY: Helps AI model handle different lighting conditions\n",
                "        # Randomly adjusts brightness, contrast, saturation, hue\n",
                "        VideoColorJitter(apply_to=video_modality.modality_keys, \n",
                "                        brightness=0.3,   # ±30% brightness change\n",
                "                        contrast=0.4,     # ±40% contrast change  \n",
                "                        saturation=0.5,   # ±50% saturation change\n",
                "                        hue=0.08),        # ±8% hue change\n",
                "        \n",
                "        # 5. Convert back to numpy arrays (for final processing)\n",
                "        VideoToNumpy(apply_to=video_modality.modality_keys),\n",
                "\n",
                "        # === ROBOT STATE TRANSFORMATIONS ===\n",
                "        # Process robot joint positions and body state information\n",
                "        \n",
                "        # 1. Convert state data to tensors\n",
                "        StateActionToTensor(apply_to=state_modality.modality_keys),\n",
                "        \n",
                "        # 2. Normalize state values to range [-1, 1]\n",
                "        # WHY: AI models work better when all numbers are in similar ranges\n",
                "        # \"min_max\" normalization scales values proportionally\n",
                "        StateActionTransform(apply_to=state_modality.modality_keys, normalization_modes={\n",
                "            key: \"min_max\" for key in state_modality.modality_keys  # Apply to all state keys\n",
                "        }),\n",
                "\n",
                "        # === ACTION TRANSFORMATIONS ===\n",
                "        # Process robot action commands (what robot should do next)\n",
                "        \n",
                "        # 1. Convert action data to tensors\n",
                "        StateActionToTensor(apply_to=action_modality.modality_keys),\n",
                "        \n",
                "        # 2. Normalize action values to range [-1, 1]\n",
                "        # WHY: Consistent scaling helps AI model learn action patterns\n",
                "        StateActionTransform(apply_to=action_modality.modality_keys, normalization_modes={\n",
                "            key: \"min_max\" for key in action_modality.modality_keys  # Apply to all action keys\n",
                "        }),\n",
                "\n",
                "        # === FINAL STEP: CONCATENATION ===\n",
                "        # Combine all the processed data into unified arrays\n",
                "        # Like putting all ingredients into organized containers\n",
                "        ConcatTransform(\n",
                "            video_concat_order=video_modality.modality_keys,    # How to order video data\n",
                "            state_concat_order=state_modality.modality_keys,    # How to order state data\n",
                "            action_concat_order=action_modality.modality_keys,  # How to order action data\n",
                "        ),\n",
                "    ]\n",
                ")\n",
                "\n",
                "print(\"✅ Transform pipeline created!\")\n",
                "print(f\"🔄 Will apply {len(to_apply_transforms.transforms)} transformations to the data\")\n",
                "print(\"\\n💡 What these transforms do:\")\n",
                "print(\"   📹 Video: Resize→Crop→ColorAdjust→Normalize\")\n",
                "print(\"   🤖 State: Convert→Normalize (robot joint positions)\")\n",
                "print(\"   🎯 Action: Convert→Normalize (robot commands)\")\n",
                "print(\"   📦 Final: Combine everything into organized arrays\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now see how the data is different after applying the transformations.\n",
                "\n",
                "e.g. states and actions are being normalized and concatenated, video images are being cropped, resized, and color-jittered."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = LeRobotSingleDataset(\n",
                "    DATA_PATH,\n",
                "    modality_configs,\n",
                "    transforms=to_apply_transforms,\n",
                "    embodiment_tag=embodiment_tag\n",
                ")\n",
                "\n",
                "# print the 7th data point\n",
                "resp = dataset[7]\n",
                "any_describe(resp)\n",
                "print(resp.keys())\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "gr00t",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
