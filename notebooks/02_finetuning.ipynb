{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7d4aac",
   "metadata": {},
   "source": [
    "# Fine-tuning Isaac GR00T N1.5 for LeRobot SO-101 Arm\n",
    "\n",
    "> _This tutorial is based on the [official Hugging Face blog post](https://huggingface.co/blog/nvidia/gr00t-n1-5-so101-tuning) by the NVIDIA team._\n",
    "\n",
    "<div style=\"border: 4px solid #f84773ff; padding: 12px; margin: 16px 0; border-radius: 4px;\">\n",
    "<strong>⚠️ Note:</strong> You don't need to run any command here. We fine-tuned the model for you. Just get familiar with fine-tuning steps.\n",
    "</div>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook provides a comprehensive tutorial on how to post-train (fine-tune) [Isaac GR00T N1.5](https://huggingface.co/nvidia/GR00T-N1.5-3B) using teleoperation data from a single SO-101 robot arm.\n",
    "\n",
    "[NVIDIA Isaac GR00T](https://developer.nvidia.com/isaac/gr00t) (Generalist Robot 00 Technology) is a research and development platform for building robot foundation models and data pipelines, designed to accelerate the creation of intelligent, adaptable robots.\n",
    "\n",
    "Isaac GR00T N1.5 is the first major update to Isaac GR00T N1, the world's first open foundation model for generalized humanoid robot reasoning and skills. This cross-embodiment model processes multimodal inputs, including language and images, to perform manipulation tasks across diverse environments. It is adaptable through post-training for specific embodiments, tasks, and environments.\n",
    "\n",
    "![GR00T Demo](https://cdn-uploads.huggingface.co/production/uploads/67b8da81d01134f89899b4a7/rbIZbAfvia_oWaztGlRbu.gif)\n",
    "\n",
    "## Step 1: Dataset Preparation\n",
    "\n",
    "Users can fine-tune GR00T N1.5 with any LeRobot dataset. For this tutorial, we'll use the [table cleanup task](https://huggingface.co/spaces/lerobot/visualize_dataset?dataset=youliangtan%2Fso100-table-cleanup) as an example.\n",
    "\n",
    "**Important**: Datasets for the SO-100 or SO-101 are not included in GR00T N1.5's initial pre-training. Therefore, we'll be training it as a `new_embodiment`.\n",
    "\n",
    "![Dataset Example](https://cdn-uploads.huggingface.co/production/uploads/67c205dafa508474d715f1d6/fu3Xd1XX4fP2TYJ6hv38Z.png)\n",
    "\n",
    "### 1.1 Download Your Dataset\n",
    "\n",
    "** Download Example Dataset**\n",
    "Download the pre-existing [so101-table-cleanup](https://huggingface.co/datasets/youliangtan/so101-table-cleanup) dataset:\n",
    "\n",
    "```bash\n",
    "huggingface-cli download \\\n",
    "    --repo-type dataset youliangtan/so101-table-cleanup \\\n",
    "    --local-dir ./demo_data/so101-table-cleanup\n",
    "```\n",
    "\n",
    "### 1.2 Configure Modality File\n",
    "\n",
    "The `modality.json` file provides essential information about state and action modalities to make the dataset \"GR00T-compatible\".\n",
    "\n",
    "**For dual-camera setup (like SO-101):**\n",
    "\n",
    "```bash\n",
    "cp getting_started/examples/so100_dualcam__modality.json ./demo_data/so101-table-cleanup/meta/modality.json\n",
    "```\n",
    "\n",
    "### 1.3 Verify Dataset Loading\n",
    "\n",
    "Test that your dataset can be loaded correctly:\n",
    "\n",
    "```bash\n",
    "python scripts/load_dataset.py \\\n",
    "    --dataset-path ./demo_data/so101-table-cleanup \\\n",
    "    --plot-state-action \\\n",
    "    --video-backend torchvision_av\n",
    "```\n",
    "\n",
    "This script will visualize the dataset and confirm it's properly formatted for GR00T training.\n",
    "\n",
    "## Step 2: Fine-tuning the Model\n",
    "\n",
    "Now we'll fine-tune GR00T N1.5 using the prepared dataset. The fine-tuning process adapts the pre-trained model to your specific robotic embodiment and tasks.\n",
    "\n",
    "### 2.1 Basic Fine-tuning Command\n",
    "\n",
    "Execute the following command to start fine-tuning:\n",
    "\n",
    "```bash\n",
    "python scripts/gr00t_finetune.py \\\n",
    "   --dataset-path ./demo_data/so101-table-cleanup/ \\\n",
    "   --num-gpus 1 \\\n",
    "   --output-dir ./so101-checkpoints  \\\n",
    "   --max-steps 10000 \\\n",
    "   --data-config so100_dualcam \\\n",
    "   --video-backend torchvision_av\n",
    "```\n",
    "\n",
    "### 2.2 Fine-tuning Parameters Explained\n",
    "\n",
    "- `--dataset-path`: Path to your prepared dataset\n",
    "- `--num-gpus`: Number of GPUs to use for training\n",
    "- `--output-dir`: Directory where model checkpoints will be saved\n",
    "- `--max-steps`: Maximum number of training steps\n",
    "- `--data-config`: Configuration matching your camera setup\n",
    "- `--video-backend`: Backend for video processing\n",
    "\n",
    "**Advanced Training Parameters:**\n",
    "\n",
    "- `--no-tune_diffusion_model`: Reduces VRAM usage by ~10GB\n",
    "- `--batch-size`: Adjust based on available memory\n",
    "- `--lora-rank`: Lower values reduce memory usage\n",
    "- `--dataloader-num-workers`: Adjust for CPU/memory balance\n",
    "\n",
    "### 2.4 Monitoring Training Progress\n",
    "\n",
    "During training, monitor:\n",
    "\n",
    "- Loss curves in the output logs\n",
    "- Checkpoint saving frequency\n",
    "- Memory usage\n",
    "- Training time estimates\n",
    "\n",
    "Training typically takes several hours depending on dataset size and hardware.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0386ad7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
